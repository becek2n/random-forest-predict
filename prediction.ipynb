{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwFnJsE6vjf8",
        "outputId": "100909ec-0294-4583-fee1-cc2870c6c081"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in d:\\source code\\project\\random-forest-predict\\venv\\lib\\site-packages (from pandas) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\source code\\project\\random-forest-predict\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting tzdata>=2022.7\n",
            "  Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Requirement already satisfied: six>=1.5 in d:\\source code\\project\\random-forest-predict\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTM-T_ynzUif",
        "outputId": "82ae4fa0-65af-4b5b-9248-3b5e79d19111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted column name for 'AC_BR': ACC_NBR\n"
          ]
        }
      ],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from collections import Counter\n",
        "\n",
        "# Sample dataset of column name variations\n",
        "column_variations = [\n",
        "    \"ACC_NUMBER\", \"ACCNBR\", \"ACC_NBR\", \"ACNMBR\",\n",
        "    \"ACCOUNT_NUMBER\", \"ACCNT_NMBR\", \"ACCNT_NBR\"\n",
        "]\n",
        "\n",
        "# Function to predict the standard column name from a new variation\n",
        "def predict_column_name(new_column_name, column_variations):\n",
        "    # Calculate the similarity of the new column name to the existing ones\n",
        "    similarities = [(col, fuzz.ratio(new_column_name, col)) for col in column_variations]\n",
        "\n",
        "    # Sort the columns by similarity score (higher score means more similar)\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the most similar column name (typically the first one after sorting)\n",
        "    predicted_name = similarities[0][0]\n",
        "\n",
        "    return predicted_name\n",
        "\n",
        "# Example: Predict for the new column name 'ACCNTNMBR'\n",
        "new_column_name = \"AC_BR\"\n",
        "predicted_name = predict_column_name(new_column_name, column_variations)\n",
        "\n",
        "print(f\"Predicted column name for '{new_column_name}': {predicted_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkZSKXpERFMY"
      },
      "source": [
        "**Using Fuzzy algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02MR-rj34tle",
        "outputId": "759aa79c-56cb-497d-e6f4-8a112da8793d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Column Names and Types:\n",
            "Account Number: VARCHAR(255)\n",
            "Date of Birth: DATE\n",
            "Phone Number: VARCHAR(255)\n",
            "PERS_ID: TEXT\n",
            "ID_SUB_CTG: TEXT\n",
            "ID_SUB_CTG: TEXT\n",
            "ID_SUB_CTG: TEXT\n",
            "\n",
            "Generated SQL CREATE TABLE Query:\n",
            "CREATE TABLE predicted_table (\n",
            "    account_number VARCHAR(255) NOT NULL,\n",
            "    date_of_birth DATE NOT NULL,\n",
            "    phone_number VARCHAR(255) NOT NULL,\n",
            "    pers_id TEXT NOT NULL,\n",
            "    id_sub_ctg TEXT NOT NULL,\n",
            "    id_sub_ctg TEXT NOT NULL,\n",
            "    id_sub_ctg TEXT NOT NULL\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Define the dataset of column name variations and their corresponding standard names\n",
        "column_variations = [\n",
        "    'CUST_NO', 'OBL_CODE', 'ACCT_NBR',\n",
        "    'CUST_TYPE',\n",
        "    'ACCOUNT_NUMBER', 'ACCNT_NMBR', 'ACCNT_NBR',\n",
        "    'CUS_NAME', 'CUSTOMER_NAME', 'CLIENT_NAME',\n",
        "    'PHN', 'PHONE_NUMBER', 'CONTACT_NUMBER',\n",
        "    'DOB', 'DATE_OF_BIRTH', 'BIRTHDATE',\n",
        "    'ADDR', 'ADDRESS', 'STREET_ADDRESS'\n",
        "]\n",
        "\n",
        "standard_names = [\n",
        "    'PERS_ID', 'PERS_ID', 'PERS_ID',\n",
        "    'ID_SUB_CTG',\n",
        "    'Account Number', 'Account Number', 'Account Number',\n",
        "    'Customer Name', 'Customer Name', 'Customer Name',\n",
        "    'Phone Number', 'Phone Number', 'Phone Number',\n",
        "    'Date of Birth', 'Date of Birth', 'Date of Birth',\n",
        "    'Street Address', 'Street Address', 'Street Address'\n",
        "]\n",
        "\n",
        "# Define new column name variations for prediction\n",
        "new_column_names = [\n",
        "    'ac_br', 'd_birth', 'con_nmbr', 'CU_NO',\n",
        "    'CUST_TYPE', 'CUSTYPE', 'CTYPE'\n",
        "    ]\n",
        "\n",
        "# A dictionary to map common abbreviations to their full names\n",
        "abbreviation_map = {\n",
        "    'ac_br': 'Account Number',\n",
        "    'd_birth': 'Date of Birth',\n",
        "    'con_nmbr': 'Phone Number',\n",
        "    'add_r': 'Street Address',\n",
        "    # Add more mappings here as necessary\n",
        "}\n",
        "\n",
        "# Function to predict the standard column name using fuzzy matching or abbreviation map\n",
        "def predict_column_name(new_name, column_variations, standard_names, abbreviation_map):\n",
        "    # Check if the new name is in the abbreviation map\n",
        "    if new_name in abbreviation_map:\n",
        "        return abbreviation_map[new_name]\n",
        "\n",
        "    # Use fuzzy matching to compare the new name with all column variations\n",
        "    similarities = [(var, fuzz.ratio(new_name, var)) for var in column_variations]\n",
        "    # Sort by similarity score in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Check for threshold similarity (to avoid incorrect matches)\n",
        "    threshold = 60  # Lowered threshold for better matching\n",
        "    if similarities[0][1] < threshold:\n",
        "        return \"Unknown\"  # If the match is not strong enough, return \"Unknown\"\n",
        "\n",
        "    # Return the standard name corresponding to the highest similarity match\n",
        "    max_similarity_idx = column_variations.index(similarities[0][0])\n",
        "    return standard_names[max_similarity_idx]\n",
        "\n",
        "# Function to predict the SQL data type based on the column name\n",
        "def predict_data_type(column_name):\n",
        "    # Map column names to SQL data types\n",
        "    column_name = column_name.lower()\n",
        "\n",
        "    if 'account' in column_name or 'phn' in column_name or 'number' in column_name:\n",
        "        return 'VARCHAR(255)'  # Account Number, Phone Number, etc. are typically VARCHAR\n",
        "    elif 'date' in column_name or 'dob' in column_name:\n",
        "        return 'DATE'  # Date fields like Date of Birth are DATE\n",
        "    elif 'address' in column_name or 'street' in column_name:\n",
        "        return 'VARCHAR(255)'  # Address fields are VARCHAR\n",
        "    else:\n",
        "        return 'TEXT'  # Default to TEXT if no specific pattern is found\n",
        "\n",
        "# Predict standard column names and their corresponding data types for the new variations\n",
        "predicted_column_names_and_types = []\n",
        "for new_column in new_column_names:\n",
        "    predicted_name = predict_column_name(new_column, column_variations, standard_names, abbreviation_map)\n",
        "\n",
        "    # If \"Unknown\" is returned, handle this case.\n",
        "    if predicted_name == \"Unknown\":\n",
        "        print(f\"Warning: Unable to predict a standard name for column '{new_column}'.\")\n",
        "        predicted_type = 'TEXT'  # Default to TEXT type if not recognized\n",
        "    else:\n",
        "        predicted_type = predict_data_type(predicted_name)\n",
        "\n",
        "    predicted_column_names_and_types.append((predicted_name, predicted_type))\n",
        "\n",
        "# SQL CREATE TABLE generation based on predicted column names and types\n",
        "def generate_create_table_query(predicted_column_names_and_types, table_name):\n",
        "    sql_query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for col_name, col_type in predicted_column_names_and_types:\n",
        "        # Convert column name to lowercase with underscores (standard SQL style)\n",
        "        sql_query += f\"    {col_name.replace(' ', '_').lower()} {col_type} NOT NULL,\\n\"\n",
        "\n",
        "    # Remove the trailing comma for the last column\n",
        "    sql_query = sql_query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Generate the SQL CREATE TABLE query based on the predicted column names and types\n",
        "table_name = \"predicted_table\"\n",
        "create_table_query = generate_create_table_query(predicted_column_names_and_types, table_name)\n",
        "\n",
        "# Output the predicted column names, data types, and the SQL query\n",
        "print(\"Predicted Column Names and Types:\")\n",
        "for col_name, col_type in predicted_column_names_and_types:\n",
        "    print(f\"{col_name}: {col_type}\")\n",
        "\n",
        "print(\"\\nGenerated SQL CREATE TABLE Query:\")\n",
        "print(create_table_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL6VfodCRz_j"
      },
      "source": [
        "**Using Random Forest with Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pQBguM6RzJr",
        "outputId": "028b0cf7-e583-45eb-cfdf-487caa57c542"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
            "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mmissing.pyx:40\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Define the dataset of column name variations and their corresponding standard names\n",
        "column_variations = [\n",
        "    'ACC_NUMBER', 'ACCNBR', 'ACC_NBR', 'ACNMBR',\n",
        "    'ACCOUNT_NUMBER', 'ACCNT_NMBR', 'ACCNT_NBR',\n",
        "    'CUS_NAME', 'CUSTOMER_NAME', 'CLIENT_NAME',\n",
        "    'PHN', 'PHONE_NUMBER', 'CONTACT_NUMBER',\n",
        "    'DOB', 'DATE_OF_BIRTH', 'BIRTHDATE',\n",
        "    'ADDR', 'ADDRESS', 'STREET_ADDRESS'\n",
        "]\n",
        "\n",
        "standard_names = [\n",
        "    'Account Number', 'Account Number', 'Account Number', 'Account Number',\n",
        "    'Account Number', 'Account Number', 'Account Number',\n",
        "    'Customer Name', 'Customer Name', 'Customer Name',\n",
        "    'Phone Number', 'Phone Number', 'Phone Number',\n",
        "    'Date of Birth', 'Date of Birth', 'Date of Birth',\n",
        "    'Street Address', 'Street Address', 'Street Address'\n",
        "]\n",
        "\n",
        "# Define new column name variations for prediction\n",
        "new_column_names = ['ac_br', 'd_birth', 'con_nmbr']\n",
        "\n",
        "# Create a DataFrame from the column variations and their corresponding standard names\n",
        "data = pd.DataFrame({\n",
        "    'column_variation': column_variations,\n",
        "    'standard_name': standard_names\n",
        "})\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data['column_variation']\n",
        "y = data['standard_name']\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline that first transforms the text data and then applies a classifier\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(),  # Converts column names into numerical features\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Now, let's use the trained model to predict new column names\n",
        "predicted_names = model.predict(new_column_names)\n",
        "\n",
        "# Function to predict SQL data type based on the column name\n",
        "def predict_data_type(column_name):\n",
        "    column_name = column_name.lower()\n",
        "\n",
        "    if 'account' in column_name or 'phn' in column_name or 'number' in column_name:\n",
        "        return 'VARCHAR(255)'  # Account Number, Phone Number, etc. are typically VARCHAR\n",
        "    elif 'date' in column_name or 'dob' in column_name:\n",
        "        return 'DATE'  # Date fields like Date of Birth are DATE\n",
        "    elif 'address' in column_name or 'street' in column_name:\n",
        "        return 'VARCHAR(255)'  # Address fields are VARCHAR\n",
        "    else:\n",
        "        return 'TEXT'  # Default to TEXT if no specific pattern is found\n",
        "\n",
        "# Predict standard names and data types for new column variations\n",
        "predicted_column_names_and_types = []\n",
        "for new_column, predicted_name in zip(new_column_names, predicted_names):\n",
        "    predicted_type = predict_data_type(predicted_name)\n",
        "    predicted_column_names_and_types.append((predicted_name, predicted_type))\n",
        "\n",
        "# SQL CREATE TABLE generation based on predicted column names and types\n",
        "def generate_create_table_query(predicted_column_names_and_types, table_name):\n",
        "    sql_query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for col_name, col_type in predicted_column_names_and_types:\n",
        "        # Convert column name to lowercase with underscores (standard SQL style)\n",
        "        sql_query += f\"    {col_name.replace(' ', '_').lower()} {col_type} NOT NULL,\\n\"\n",
        "\n",
        "    # Remove the trailing comma for the last column\n",
        "    sql_query = sql_query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Generate the SQL CREATE TABLE query based on the predicted column names and types\n",
        "table_name = \"predicted_table\"\n",
        "create_table_query = generate_create_table_query(predicted_column_names_and_types, table_name)\n",
        "\n",
        "# Output the predicted column names, data types, and the SQL query\n",
        "print(\"\\nPredicted Column Names and Types:\")\n",
        "for col_name, col_type in predicted_column_names_and_types:\n",
        "    print(f\"{col_name}: {col_type}\")\n",
        "\n",
        "print(\"\\nGenerated SQL CREATE TABLE Query:\")\n",
        "print(create_table_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPpuvYLhdZxy"
      },
      "source": [
        "**Random Forest without Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Kot4VzMUdZGZ",
        "outputId": "5ff41290-3cd7-43ce-bfa2-8f979f6a5c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted Column Names and Types:\n",
            "Account Number: VARCHAR(255)\n",
            "Date of Birth: DATE\n",
            "Phone Number: VARCHAR(255)\n",
            "PERS_ID: TEXT\n",
            "\n",
            "Generated SQL CREATE TABLE Query:\n",
            "CREATE TABLE predicted_table (\n",
            "    account_number VARCHAR(255) NOT NULL,\n",
            "    date_of_birth DATE NOT NULL,\n",
            "    phone_number VARCHAR(255) NOT NULL,\n",
            "    pers_id TEXT NOT NULL\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "\n",
        "# Define the dataset of column name variations and their corresponding standard names\n",
        "column_variations = [\n",
        "    'CUST_NO', 'OBL_CODE', 'ACCT_NBR',\n",
        "    'CUST_TYPE',\n",
        "    'ACC_NUMBER', 'ACCNBR', 'ACC_NBR', 'ACNMBR',\n",
        "    'ACCOUNT_NUMBER', 'ACCNT_NMBR', 'ACCNT_NBR',\n",
        "    'CUS_NAME', 'CUSTOMER_NAME', 'CLIENT_NAME',\n",
        "    'PHN', 'PHONE_NUMBER', 'CONTACT_NUMBER',\n",
        "    'DOB', 'DATE_OF_BIRTH', 'BIRTHDATE',\n",
        "    'ADDR', 'ADDRESS', 'STREET_ADDRESS',\n",
        "    'AC_BR', 'D_BIRTH', 'CON_NMBR'  # Additional variations added for the new prediction\n",
        "]\n",
        "\n",
        "standard_names = [\n",
        "    'PERS_ID', 'PERS_ID', 'PERS_ID',\n",
        "    'ID_SUB_CTG',\n",
        "    'Account Number', 'Account Number', 'Account Number', 'Account Number',\n",
        "    'Account Number', 'Account Number', 'Account Number',\n",
        "    'Customer Name', 'Customer Name', 'Customer Name',\n",
        "    'Phone Number', 'Phone Number', 'Phone Number',\n",
        "    'Date of Birth', 'Date of Birth', 'Date of Birth',\n",
        "    'Street Address', 'Street Address', 'Street Address',\n",
        "    'Account Number', 'Date of Birth', 'Phone Number'  # These correspond to the new variations\n",
        "]\n",
        "\n",
        "# Define new column name variations for prediction\n",
        "new_column_names = ['ac_br', 'd_birth', 'con_nmbr', 'OBL_CODE']\n",
        "\n",
        "# Create a DataFrame from the column variations and their corresponding standard names\n",
        "data = pd.DataFrame({\n",
        "    'column_variation': column_variations,\n",
        "    'standard_name': standard_names\n",
        "})\n",
        "\n",
        "# Create a pipeline that first transforms the text data and then applies a classifier\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(),  # Converts column names into numerical features\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
        ")\n",
        "\n",
        "# Train the model on all the available data\n",
        "model.fit(data['column_variation'], data['standard_name'])\n",
        "\n",
        "# Now, let's use the trained model to predict new column names\n",
        "predicted_names = model.predict(new_column_names)\n",
        "\n",
        "# Function to predict SQL data type based on the column name\n",
        "def predict_data_type(column_name):\n",
        "    column_name = column_name.lower()\n",
        "\n",
        "    if 'account' in column_name or 'phn' in column_name or 'number' in column_name:\n",
        "        return 'VARCHAR(255)'  # Account Number, Phone Number, etc. are typically VARCHAR\n",
        "    elif 'date' in column_name or 'dob' in column_name:\n",
        "        return 'DATE'  # Date fields like Date of Birth are DATE\n",
        "    elif 'address' in column_name or 'street' in column_name:\n",
        "        return 'VARCHAR(255)'  # Address fields are VARCHAR\n",
        "    else:\n",
        "        return 'TEXT'  # Default to TEXT if no specific pattern is found\n",
        "\n",
        "# Predict standard names and data types for new column variations\n",
        "predicted_column_names_and_types = []\n",
        "for new_column, predicted_name in zip(new_column_names, predicted_names):\n",
        "    predicted_type = predict_data_type(predicted_name)\n",
        "    predicted_column_names_and_types.append((predicted_name, predicted_type))\n",
        "\n",
        "# SQL CREATE TABLE generation based on predicted column names and types\n",
        "def generate_create_table_query(predicted_column_names_and_types, table_name):\n",
        "    sql_query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for col_name, col_type in predicted_column_names_and_types:\n",
        "        # Convert column name to lowercase with underscores (standard SQL style)\n",
        "        sql_query += f\"    {col_name.replace(' ', '_').lower()} {col_type} NOT NULL,\\n\"\n",
        "\n",
        "    # Remove the trailing comma for the last column\n",
        "    sql_query = sql_query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Generate the SQL CREATE TABLE query based on the predicted column names and types\n",
        "table_name = \"predicted_table\"\n",
        "create_table_query = generate_create_table_query(predicted_column_names_and_types, table_name)\n",
        "\n",
        "# Output the predicted column names, data types, and the SQL query\n",
        "print(\"\\nPredicted Column Names and Types:\")\n",
        "for col_name, col_type in predicted_column_names_and_types:\n",
        "    print(f\"{col_name}: {col_type}\")\n",
        "\n",
        "print(\"\\nGenerated SQL CREATE TABLE Query:\")\n",
        "print(create_table_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0brRCXlQgNoA"
      },
      "source": [
        "Show Diagram Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "z5RUNTrbfg4_",
        "outputId": "2abe2245-c5d1-4e49-df1a-781115acad62"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the feature importances from the trained model\n",
        "importances = model.named_steps['randomforestclassifier'].feature_importances_\n",
        "\n",
        "# Get the feature names from the TF-IDF vectorizer (vocabulary)\n",
        "vectorizer = model.named_steps['tfidfvectorizer']\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for feature importances and their corresponding feature names\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title('Feature Importance in Random Forest Model')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9X4czPrgY4Q"
      },
      "source": [
        "Show Diagram Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oT9SjloUgThv",
        "outputId": "e2c22510-7f1d-4cc1-97f2-a5cfd79fc92f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Get the trained Random Forest model\n",
        "rf_model = model.named_steps['randomforestclassifier']\n",
        "\n",
        "# Loop through the first 3 trees in the Random Forest and visualize them\n",
        "for i in range(3):  # You can change this number to visualize more trees\n",
        "    tree = rf_model.estimators_[i]\n",
        "\n",
        "    # Export tree to DOT format\n",
        "    dot_data = export_graphviz(tree,\n",
        "                               feature_names=model.named_steps['tfidfvectorizer'].get_feature_names_out(),\n",
        "                               filled=True,\n",
        "                               max_depth=3,  # Limit the depth of the tree for better readability\n",
        "                               impurity=False,\n",
        "                               proportion=True)\n",
        "\n",
        "    # Generate and display the tree graph\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    graph.render(f\"tree_{i}.pdf\", view=True)  # Optionally save the graph as a PDF and open it\n",
        "    display(graph)  # Display the graph inline (works in Jupyter Notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest with Training Dataset from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SOURCE_FIELD_NAME  FIELD_NAME\n",
            "0           CUST_NO     PERS_ID\n",
            "1          OBL_CODE     PERS_ID\n",
            "2          ACCT_NBR     PERS_ID\n",
            "3           CUST_NO     PERS_ID\n",
            "4         CUST_TYPE  ID_SUB_CTG\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SOURCE_FIELD_NAME</th>\n",
              "      <th>FIELD_NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CUST_NO</td>\n",
              "      <td>PERS_ID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OBL_CODE</td>\n",
              "      <td>PERS_ID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACCT_NBR</td>\n",
              "      <td>PERS_ID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CUST_NO</td>\n",
              "      <td>PERS_ID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CUST_TYPE</td>\n",
              "      <td>ID_SUB_CTG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  SOURCE_FIELD_NAME  FIELD_NAME\n",
              "0           CUST_NO     PERS_ID\n",
              "1          OBL_CODE     PERS_ID\n",
              "2          ACCT_NBR     PERS_ID\n",
              "3           CUST_NO     PERS_ID\n",
              "4         CUST_TYPE  ID_SUB_CTG"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = dataFrame['SOURCE_FIELD_NAME']\n",
        "y = dataFrame['FIELD_NAME']\n",
        "\n",
        "dataFrame.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 40.52%\n",
            "\n",
            "Predicted Column Names and Types:\n",
            "CUST_TY: TEXT\n",
            "GCIF_NO: TEXT\n",
            "PERS_ID: TEXT\n",
            "\n",
            "Generated SQL CREATE TABLE Query:\n",
            "CREATE TABLE predicted_table (\n",
            "    cust_ty TEXT NOT NULL,\n",
            "    gcif_no TEXT NOT NULL,\n",
            "    pers_id TEXT NOT NULL\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "dataFrame = pd.read_csv('dataset-clean.csv')\n",
        "\n",
        "\n",
        "# Define new column name variations for prediction\n",
        "new_column_names = ['CUST_TYPE', 'DCIF', 'cust_no']\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a pipeline that first transforms the text data and then applies a classifier\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(),  # Converts column names into numerical features\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Now, let's use the trained model to predict new column names\n",
        "predicted_names = model.predict(new_column_names)\n",
        "\n",
        "# Function to predict SQL data type based on the column name\n",
        "def predict_data_type(column_name):\n",
        "    column_name = column_name.lower()\n",
        "\n",
        "    if 'account' in column_name or 'phn' in column_name or 'number' in column_name:\n",
        "        return 'VARCHAR(255)'  # Account Number, Phone Number, etc. are typically VARCHAR\n",
        "    elif 'date' in column_name or 'dob' in column_name:\n",
        "        return 'DATE'  # Date fields like Date of Birth are DATE\n",
        "    elif 'address' in column_name or 'street' in column_name:\n",
        "        return 'VARCHAR(255)'  # Address fields are VARCHAR\n",
        "    else:\n",
        "        return 'TEXT'  # Default to TEXT if no specific pattern is found\n",
        "\n",
        "# Predict standard names and data types for new column variations\n",
        "predicted_column_names_and_types = []\n",
        "for new_column, predicted_name in zip(new_column_names, predicted_names):\n",
        "    predicted_type = predict_data_type(predicted_name)\n",
        "    predicted_column_names_and_types.append((predicted_name, predicted_type))\n",
        "\n",
        "# SQL CREATE TABLE generation based on predicted column names and types\n",
        "def generate_create_table_query(predicted_column_names_and_types, table_name):\n",
        "    sql_query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for col_name, col_type in predicted_column_names_and_types:\n",
        "        # Convert column name to lowercase with underscores (standard SQL style)\n",
        "        sql_query += f\"    {col_name.replace(' ', '_').lower()} {col_type} NOT NULL,\\n\"\n",
        "\n",
        "    # Remove the trailing comma for the last column\n",
        "    sql_query = sql_query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Generate the SQL CREATE TABLE query based on the predicted column names and types\n",
        "table_name = \"predicted_table\"\n",
        "create_table_query = generate_create_table_query(predicted_column_names_and_types, table_name)\n",
        "\n",
        "# Output the predicted column names, data types, and the SQL query\n",
        "print(\"\\nPredicted Column Names and Types:\")\n",
        "for col_name, col_type in predicted_column_names_and_types:\n",
        "    print(f\"{col_name}: {col_type}\")\n",
        "\n",
        "print(\"\\nGenerated SQL CREATE TABLE Query:\")\n",
        "print(create_table_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OPTIMIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SOURCE_FIELD_NAME  FIELD_NAME\n",
            "0           CUST_NO     PERS_ID\n",
            "1          OBL_CODE     PERS_ID\n",
            "2          ACCT_NBR     PERS_ID\n",
            "3           CUST_NO     PERS_ID\n",
            "4         CUST_TYPE  ID_SUB_CTG\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "dataFrame = pd.read_csv('dataset-clean.csv')\n",
        "\n",
        "# Define your feature set and target\n",
        "# Assuming 'X' contains the feature columns and 'y' contains the target column.\n",
        "# Make sure your feature columns are of type 'string' for text-based features.\n",
        "X = dataFrame['SOURCE_FIELD_NAME'].astype(str)  # Replace 'column_name' with the actual name of your feature column\n",
        "y = dataFrame['FIELD_NAME']  # Replace 'target_column' with the actual name of your target column\n",
        "\n",
        "print(dataFrame.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0             CUST_NO\n",
              "1            OBL_CODE\n",
              "2            ACCT_NBR\n",
              "3             CUST_NO\n",
              "4           CUST_TYPE\n",
              "            ...      \n",
              "3468      MT_STATE_CD\n",
              "3469           CIF_ID\n",
              "3470          GCIF_NO\n",
              "3471               NM\n",
              "3472    DT_REGISTERED\n",
              "Name: SOURCE_FIELD_NAME, Length: 3473, dtype: object"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = X.astype(str)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming 'X' is a pandas DataFrame containing text columns\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_transformed = vectorizer.fit_transform(X)  # Replace 'text_column' with your actual text column\n",
        "Y_transformed = vectorizer.fit_transform(y)  # Replace 'text_column' with your actual text column\n",
        "X_test_transformed = vectorizer.fit_transform(X_test)  # Replace 'text_column' with your actual text column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 660, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 832, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\pandas\\core\\series.py\", line 1031, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'CUST_NO'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m]\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit on the transformed features\u001b[39;00m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1000\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    998\u001b[0m     )\n\u001b[1;32m-> 1000\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 660, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n    X, y = validate_data(\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n    X = check_array(\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 832, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"d:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\pandas\\core\\series.py\", line 1031, in __array__\n    arr = np.asarray(values, dtype=dtype)\nValueError: could not convert string to float: 'CUST_NO'\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('classifier', RandomForestClassifier())  # Use the appropriate classifier\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X, y)  # Fit on the transformed features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
            "Best estimator: Pipeline(steps=[('classifier', RandomForestClassifier(n_estimators=200))])\n",
            "Best Score: 0.39101859722596555\n"
          ]
        }
      ],
      "source": [
        "# Access best parameters and the model\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best estimator: {grid_search.best_estimator_}\")\n",
        "# Display the best score achieved by GridSearchCV\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_test_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 305 features, but RandomForestClassifier is expecting 2228 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# accuracy = accuracy_score(y_test, y_pred)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\u001b[39;00m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:597\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    596\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\pipeline.py:786\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    785\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[0;32m    789\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:946\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    944\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    949\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:638\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    636\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: X has 305 features, but RandomForestClassifier is expecting 2228 features as input."
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = grid_search.predict(X_test_transformed)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'lower'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Reshape new_column_names to 2D (1 sample per row)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m new_column_names_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(new_column_names)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m new_column_names_reshaped_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_column_names_reshaped\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace 'text_column' with your actual text column\u001b[39;00m\n\u001b[0;32m     10\u001b[0m new_column_names_reshaped_transformed\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Predict column names using the trained model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# predicted_names = grid_search.predict(new_column_names_reshaped_transformed)\u001b[39;00m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:104\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
            "File \u001b[1;32md:\\Source Code\\project\\random-forest-predict\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:62\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 62\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Predicting new column names (ensure that new_column_names is in a suitable format)\n",
        "new_column_names = ['CUST_TYPE', 'DCIF', 'cust_no']\n",
        "new_column_names = [str(col) for col in new_column_names]  # Ensure all column names are strings\n",
        "\n",
        "# Reshape new_column_names to 2D (1 sample per row)\n",
        "new_column_names_reshaped = np.array(new_column_names).reshape(-1, 1)\n",
        "\n",
        "new_column_names_reshaped_transformed = vectorizer.fit_transform(new_column_names_reshaped)  # Replace 'text_column' with your actual text column\n",
        "new_column_names_reshaped_transformed\n",
        "# Predict column names using the trained model\n",
        "# predicted_names = grid_search.predict(new_column_names_reshaped_transformed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 652, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 586, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1376, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1263, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 62, in _preprocess\n    doc = doc.lower()\nAttributeError: 'numpy.ndarray' object has no attribute 'lower'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[84], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Fit the model with grid search\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Access best parameters and the model\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1000\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    998\u001b[0m     )\n\u001b[1;32m-> 1000\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 652, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 586, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1376, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1263, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 62, in _preprocess\n    doc = doc.lower()\nAttributeError: 'numpy.ndarray' object has no attribute 'lower'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "dataFrame = pd.read_csv('dataset-clean.csv')\n",
        "\n",
        "# Define your feature set and target\n",
        "# Assuming 'X' contains the feature columns and 'y' contains the target column.\n",
        "# Make sure your feature columns are of type 'string' for text-based features.\n",
        "X = dataFrame['SOURCE_FIELD_NAME'].astype(str)  # Replace 'column_name' with the actual name of your feature column\n",
        "y = dataFrame['FIELD_NAME']  # Replace 'target_column' with the actual name of your target column\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data to 2D if it's a single column (1D)\n",
        "X_train = X_train.values.reshape(-1, 1)  # Reshaping to 2D (number_of_samples, 1_feature)\n",
        "X_test = X_test.values.reshape(-1, 1)  # Reshaping to 2D (number_of_samples, 1_feature)\n",
        "\n",
        "# Create a pipeline that first transforms the text data and then applies a classifier\n",
        "model = make_pipeline(\n",
        "    SimpleImputer(strategy='most_frequent'),  # Handle missing values if any\n",
        "    TfidfVectorizer(),  # Converts text into numerical features\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
        ")\n",
        "\n",
        "# Define parameter grid for grid search\n",
        "param_grid = {\n",
        "    'randomforestclassifier__n_estimators': [100, 200],\n",
        "    'randomforestclassifier__max_depth': [10, 20, None],\n",
        "    'randomforestclassifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Access best parameters and the model\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best estimator: {grid_search.best_estimator_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 652, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 586, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1376, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1263, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 62, in _preprocess\n    doc = doc.lower()\nAttributeError: 'numpy.ndarray' object has no attribute 'lower'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Fit the model with grid search\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Access best parameters and the model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1000\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    998\u001b[0m     )\n\u001b[1;32m-> 1000\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[1;32mc:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 36 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n36 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 652, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 586, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2104, in fit_transform\n    X = super().fit_transform(raw_documents)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1376, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1263, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\Djanuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 62, in _preprocess\n    doc = doc.lower()\nAttributeError: 'numpy.ndarray' object has no attribute 'lower'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "dataFrame = pd.read_csv('dataset-clean.csv')\n",
        "\n",
        "# Define your feature set and target\n",
        "# Assuming 'X' contains the feature columns and 'y' contains the target column.\n",
        "# Make sure your feature columns are of type 'string' for text-based features.\n",
        "X = dataFrame['SOURCE_FIELD_NAME'].astype(str)  # Replace 'column_name' with the actual name of your feature column\n",
        "y = dataFrame['FIELD_NAME']  # Replace 'target_column' with the actual name of your target column\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data to 2D if it's a single column (1D)\n",
        "X_train = X_train.values.reshape(-1, 1)  # Reshaping to 2D (number_of_samples, 1_feature)\n",
        "X_test = X_test.values.reshape(-1, 1)  # Reshaping to 2D (number_of_samples, 1_feature)\n",
        "\n",
        "# Create a pipeline that first transforms the text data and then applies a classifier\n",
        "model = make_pipeline(\n",
        "    SimpleImputer(strategy='most_frequent'),  # Handle missing values if any\n",
        "    TfidfVectorizer(),  # Converts text into numerical features\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
        ")\n",
        "\n",
        "# Define parameter grid for grid search\n",
        "param_grid = {\n",
        "    'randomforestclassifier__n_estimators': [100, 200],\n",
        "    'randomforestclassifier__max_depth': [10, 20, None],\n",
        "    'randomforestclassifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Access best parameters and the model\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best estimator: {grid_search.best_estimator_}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predicting new column names (ensure that new_column_names is in a suitable format)\n",
        "new_column_names = ['CUST_TYPE', 'DCIF', 'cust_no']\n",
        "new_column_names = [str(col) for col in new_column_names]  # Ensure all column names are strings\n",
        "\n",
        "# Reshape new_column_names to 2D (1 sample per row)\n",
        "new_column_names_reshaped = np.array(new_column_names).reshape(-1, 1)\n",
        "\n",
        "# Predict column names using the trained model\n",
        "predicted_names = grid_search.predict(new_column_names_reshaped)\n",
        "\n",
        "# Function to predict SQL data type based on the column name\n",
        "def predict_data_type(column_name):\n",
        "    column_name = column_name.lower()\n",
        "\n",
        "    if 'account' in column_name or 'phn' in column_name or 'number' in column_name:\n",
        "        return 'VARCHAR(255)'  # Account Number, Phone Number, etc. are typically VARCHAR\n",
        "    elif 'date' in column_name or 'dob' in column_name:\n",
        "        return 'DATE'  # Date fields like Date of Birth are DATE\n",
        "    elif 'address' in column_name or 'street' in column_name:\n",
        "        return 'VARCHAR(255)'  # Address fields are VARCHAR\n",
        "    else:\n",
        "        return 'TEXT'  # Default to TEXT if no specific pattern is found\n",
        "\n",
        "# Predict standard names and data types for new column variations\n",
        "predicted_column_names_and_types = []\n",
        "for new_column, predicted_name in zip(new_column_names, predicted_names):\n",
        "    predicted_type = predict_data_type(predicted_name)\n",
        "    predicted_column_names_and_types.append((predicted_name, predicted_type))\n",
        "\n",
        "# SQL CREATE TABLE generation based on predicted column names and types\n",
        "def generate_create_table_query(predicted_column_names_and_types, table_name):\n",
        "    sql_query = f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    for col_name, col_type in predicted_column_names_and_types:\n",
        "        # Convert column name to lowercase with underscores (standard SQL style)\n",
        "        sql_query += f\"    {col_name.replace(' ', '_').lower()} {col_type} NOT NULL,\\n\"\n",
        "\n",
        "    # Remove the trailing comma for the last column\n",
        "    sql_query = sql_query.rstrip(',\\n') + \"\\n);\"\n",
        "\n",
        "    return sql_query\n",
        "\n",
        "# Generate the SQL CREATE TABLE query based on the predicted column names and types\n",
        "table_name = \"predicted_table\"\n",
        "create_table_query = generate_create_table_query(predicted_column_names_and_types, table_name)\n",
        "\n",
        "# Output the predicted column names, data types, and the SQL query\n",
        "print(\"\\nPredicted Column Names and Types:\")\n",
        "for col_name, col_type in predicted_column_names_and_types:\n",
        "    print(f\"{col_name}: {col_type}\")\n",
        "\n",
        "print(\"\\nGenerated SQL CREATE TABLE Query:\")\n",
        "print(create_table_query)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
